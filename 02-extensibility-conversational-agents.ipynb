{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-agentic-frameworks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory\n",
    "target_directory = r\"C:\\Users\\pablosal\\Desktop\\gbbai-azure-ai-agentic-frameworks\"  # change your directory here\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(target_directory):\n",
    "    # Change the current working directory\n",
    "    os.chdir(target_directory)\n",
    "    print(f\"Directory changed to {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"Directory {target_directory} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending ConversableAgent: A Study in Autogen Extensibility\n",
    "\n",
    "The preceding discussion illuminated the principle of software extensibility through the lens of the ConversableAgent framework. At its core, software extensibility is the capacity of a system to incorporate additional features with minimal alterations to the established codebase. This attribute is indispensable for the sustainable growth and scalability of software architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mConversableAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msystem_message\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'You are a helpful AI Assistant.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mis_termination_msg\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_consecutive_auto_reply\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mhuman_input_mode\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ALWAYS'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NEVER'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TERMINATE'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'TERMINATE'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfunction_map\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcode_execution_config\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mllm_config\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdefault_auto_reply\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdescription\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mchat_messages\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mautogen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magentchat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAgent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "(In preview) A class for generic conversable agents which can be configured as assistant or user proxy.\n",
      "\n",
      "After receiving each message, the agent will send a reply to the sender unless the msg is a termination msg.\n",
      "For example, AssistantAgent and UserProxyAgent are subclasses of this class,\n",
      "configured with different default settings.\n",
      "\n",
      "To modify auto reply, override `generate_reply` method.\n",
      "To disable/enable human response in every turn, set `human_input_mode` to \"NEVER\" or \"ALWAYS\".\n",
      "To modify the way to get human input, override `get_human_input` method.\n",
      "To modify the way to execute code blocks, single code block, or function call, override `execute_code_blocks`,\n",
      "`run_code`, and `execute_function` methods respectively.\n",
      "\u001b[1;31mInit docstring:\u001b[0m\n",
      "Args:\n",
      "    name (str): name of the agent.\n",
      "    system_message (str or list): system message for the ChatCompletion inference.\n",
      "    is_termination_msg (function): a function that takes a message in the form of a dictionary\n",
      "        and returns a boolean value indicating if this received message is a termination message.\n",
      "        The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "    max_consecutive_auto_reply (int): the maximum number of consecutive auto replies.\n",
      "        default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\n",
      "        When set to 0, no auto reply will be generated.\n",
      "    human_input_mode (str): whether to ask for human inputs every time a message is received.\n",
      "        Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "        (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "            Under this mode, the conversation stops when the human input is \"exit\",\n",
      "            or when is_termination_msg is True and there is no human input.\n",
      "        (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "            the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "        (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "            when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "    function_map (dict[str, callable]): Mapping function names (passed to openai) to callable functions, also used for tool calls.\n",
      "    code_execution_config (dict or False): config for the code execution.\n",
      "        To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:\n",
      "        - work_dir (Optional, str): The working directory for the code execution.\n",
      "            If None, a default working directory will be used.\n",
      "            The default working directory is the \"extensions\" directory under\n",
      "            \"path_to_autogen\".\n",
      "        - use_docker (Optional, list, str or bool): The docker image to use for code execution.\n",
      "            Default is True, which means the code will be executed in a docker container. A default list of images will be used.\n",
      "            If a list or a str of image name(s) is provided, the code will be executed in a docker container\n",
      "            with the first image successfully pulled.\n",
      "            If False, the code will be executed in the current environment.\n",
      "            We strongly recommend using docker for code execution.\n",
      "        - timeout (Optional, int): The maximum execution time in seconds.\n",
      "        - last_n_messages (Experimental, int or str): The number of messages to look back for code execution.\n",
      "            If set to 'auto', it will scan backwards through all messages arriving since the agent last spoke, which is typically the last time execution was attempted. (Default: auto)\n",
      "    llm_config (dict or False or None): llm inference configuration.\n",
      "        Please refer to [OpenAIWrapper.create](/docs/reference/oai/client#create)\n",
      "        for available options.\n",
      "        When using OpenAI or Azure OpenAI endpoints, please specify a non-empty 'model' either in `llm_config` or in each config of 'config_list' in `llm_config`.\n",
      "        To disable llm-based auto reply, set to False.\n",
      "        When set to None, will use self.DEFAULT_CONFIG, which defaults to False.\n",
      "    default_auto_reply (str or dict): default auto reply when no code execution or llm-based reply is generated.\n",
      "    description (str): a short description of the agent. This description is used by other agents\n",
      "        (e.g. the GroupChatManager) to decide when to call upon this agent. (Default: system_message)\n",
      "    chat_messages (dict or None): the previous chat messages that this agent had in the past with other agents.\n",
      "        Can be used to give the agent a memory by providing the chat history. This will allow the agent to\n",
      "        resume previous had conversations. Defaults to an empty chat history.\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\pablosal\\appdata\\local\\anaconda3\\envs\\agentic-framework\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py\n",
      "\u001b[1;31mType:\u001b[0m           _ProtocolMeta\n",
      "\u001b[1;31mSubclasses:\u001b[0m     AssistantAgent, GroupChatManager, UserProxyAgent, SuperConversableAgent, MasterConversableAgent"
     ]
    }
   ],
   "source": [
    "ConversableAgent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent, AssistantAgent\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "from typing import Dict, List, Optional, Union, Callable, Literal, Any\n",
    "import os\n",
    "from utils.ml_logging import get_logger\n",
    "from datetime import datetime\n",
    "\n",
    "def get_llm_config(\n",
    "    azure_openai_key: Optional[str] = None,\n",
    "    azure_aoai_chat_model_name_deployment_id: Optional[str] = None,\n",
    "    azure_openai_api_endpoint: Optional[str] = None,\n",
    "    azure_openai_api_version: Optional[str] = None\n",
    ") -> Dict[str, List[Dict[str, str]]]:\n",
    "    \"\"\"\n",
    "    Generate a configuration list dictionary for the LLM from provided parameters or environment variables.\n",
    "\n",
    "    Args:\n",
    "        azure_openai_key (Optional[str]): The Azure OpenAI key.\n",
    "        azure_aoai_chat_model_name_deployment_id (Optional[str]): The Azure AOAI chat model name deployment ID.\n",
    "        azure_openai_api_endpoint (Optional[str]): The Azure OpenAI API endpoint.\n",
    "        azure_openai_api_version (Optional[str]): The Azure OpenAI API version.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[Dict[str, str]]]: A dictionary containing the configuration list.\n",
    "    \"\"\"\n",
    "    azure_openai_key = azure_openai_key or os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "    azure_aoai_chat_model_name_deployment_id = azure_aoai_chat_model_name_deployment_id or os.getenv(\"AZURE_AOAI_CHAT_MODEL_NAME_DEPLOYMENT_ID\")\n",
    "    azure_openai_api_endpoint = azure_openai_api_endpoint or os.getenv(\"AZURE_OPENAI_API_ENDPOINT\")\n",
    "    azure_openai_api_version = azure_openai_api_version or os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "    return {\n",
    "        \"config_list\": [\n",
    "            {\n",
    "                \"model\": azure_aoai_chat_model_name_deployment_id,\n",
    "                \"api_type\": \"azure\",\n",
    "                \"api_key\": azure_openai_key,\n",
    "                \"base_url\": azure_openai_api_endpoint,\n",
    "                \"api_version\": azure_openai_api_version\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "class SuperConversableAgent(ConversableAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent_from_yaml: Optional[str] = None,\n",
    "        name: Optional[str] = None,\n",
    "        system_message: Optional[Union[str, List]] = \"You are a helpful AI Assistant.\",\n",
    "        is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n",
    "        max_consecutive_auto_reply: Optional[int] = None,\n",
    "        human_input_mode: Literal[\"ALWAYS\", \"NEVER\", \"TERMINATE\"] = \"TERMINATE\",\n",
    "        function_map: Optional[Dict[str, Callable]] = None,\n",
    "        code_execution_config: Union[Dict, Literal[False]] = False,\n",
    "        llm_config: Optional[Union[Dict, Literal[False]]] = None,\n",
    "        default_auto_reply: Union[str, Dict] = \"\",\n",
    "        description: Optional[str] = None,\n",
    "        chat_messages: Optional[Dict[ConversableAgent, List[Dict]]] = None,\n",
    "        avatar: str = 'ðŸ¤–',\n",
    "        enable_logs: bool = True,\n",
    "        log_dir: Optional[str] = None,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the SuperConversableAgent with LLM configuration.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the agent.\n",
    "                Example: \"AssistantBot\"\n",
    "            \n",
    "            system_message (str or list): System message for the ChatCompletion inference.\n",
    "                Example: \"You are a helpful AI Assistant.\"\n",
    "            \n",
    "            is_termination_msg (function): A function that takes a message in the form of a dictionary\n",
    "                and returns a boolean value indicating if this received message is a termination message.\n",
    "                The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
    "                Example:\n",
    "                    def is_termination(message: Dict) -> bool:\n",
    "                        return message.get(\"content\") == \"exit\"\n",
    "            \n",
    "            max_consecutive_auto_reply (int): The maximum number of consecutive auto replies.\n",
    "                Defaults to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\n",
    "                When set to 0, no auto reply will be generated.\n",
    "                Example: 5\n",
    "            \n",
    "            human_input_mode (str): Whether to ask for human inputs every time a message is received.\n",
    "                Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
    "                (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
    "                    Under this mode, the conversation stops when the human input is \"exit\",\n",
    "                    or when is_termination_msg is True and there is no human input.\n",
    "                (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
    "                    the number of auto reply reaches the max_consecutive_auto_reply.\n",
    "                (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
    "                    when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
    "                Example: \"TERMINATE\"\n",
    "            \n",
    "            function_map (dict[str, callable]): Mapping function names (passed to openai) to callable functions, also used for tool calls.\n",
    "                Example: {\"get_weather\": get_weather_function}\n",
    "            \n",
    "            code_execution_config (dict or False): Config for the code execution.\n",
    "                To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:\n",
    "                - work_dir (Optional, str): The working directory for the code execution.\n",
    "                    If None, a default working directory will be used.\n",
    "                    The default working directory is the \"extensions\" directory under \"path_to_autogen\".\n",
    "                - use_docker (Optional, list, str or bool): The docker image to use for code execution.\n",
    "                    Default is True, which means the code will be executed in a docker container. A default list of images will be used.\n",
    "                    If a list or a str of image name(s) is provided, the code will be executed in a docker container\n",
    "                    with the first image successfully pulled.\n",
    "                    If False, the code will be executed in the current environment.\n",
    "                    We strongly recommend using docker for code execution.\n",
    "                - timeout (Optional, int): The maximum execution time in seconds.\n",
    "                - last_n_messages (Experimental, int or str): The number of messages to look back for code execution.\n",
    "                    If set to 'auto', it will scan backwards through all messages arriving since the agent last spoke, which is typically the last time execution was attempted. (Default: auto)\n",
    "                Example:\n",
    "                    {\n",
    "                        \"work_dir\": \"/path/to/work_dir\",\n",
    "                        \"use_docker\": True,\n",
    "                        \"timeout\": 60,\n",
    "                        \"last_n_messages\": \"auto\"\n",
    "                    }\n",
    "            \n",
    "            llm_config (dict or False or None): LLM inference configuration.\n",
    "                Please refer to [OpenAIWrapper.create](/docs/reference/oai/client#create) for available options.\n",
    "                When using OpenAI or Azure OpenAI endpoints, please specify a non-empty 'model' either in `llm_config` or in each config of 'config_list' in `llm_config`.\n",
    "                To disable llm-based auto reply, set to False.\n",
    "                When set to None, will use self.DEFAULT_CONFIG, which defaults to False.\n",
    "                Example:\n",
    "                    {\n",
    "                        \"model\": \"gpt-3.5-turbo\",\n",
    "                        \"temperature\": 0.7\n",
    "                    }\n",
    "            \n",
    "            default_auto_reply (str or dict): Default auto reply when no code execution or llm-based reply is generated.\n",
    "                Example: \"I'm not sure how to help with that.\"\n",
    "            \n",
    "            description (str): A short description of the agent. This description is used by other agents\n",
    "                (e.g. the GroupChatManager) to decide when to call upon this agent. (Default: system_message)\n",
    "                Example: \"This agent assists with weather-related queries.\"\n",
    "            \n",
    "            chat_messages (dict or None): The previous chat messages that this agent had in the past with other agents.\n",
    "                Can be used to give the agent a memory by providing the chat history. This will allow the agent to\n",
    "                resume previous conversations. Defaults to an empty chat history.\n",
    "                Example:\n",
    "                    {\n",
    "                        other_agent: [\n",
    "                            {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "                            {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n",
    "                        ]\n",
    "                    }\n",
    "            \n",
    "            avatar (str): The avatar for the agent. Defaults to a robot emoji.\n",
    "                Example: \"ðŸ¤–\"\n",
    "            \n",
    "            logging (bool): Whether to log the agent's activities. Defaults to True.\n",
    "                Example: True\n",
    "            \n",
    "            verbose (bool): Whether the agent should process messages silently. Defaults to False.\n",
    "                Example: False\n",
    "        \"\"\"\n",
    "\n",
    "        if agent_from_yaml:\n",
    "            config = self.load_agent_from_yaml(agent_from_yaml)\n",
    "            llm_config = self._resolve_llm_config(config, llm_config)\n",
    "            super().__init__(\n",
    "                name=config.get('name', 'DefaultAgentName'),\n",
    "                system_message=config.get('system_message', [\"You are a helpful AI Assistant.\"]),\n",
    "                is_termination_msg=config.get('is_termination_msg', None),\n",
    "                max_consecutive_auto_reply=config.get('max_consecutive_auto_reply', 1),\n",
    "                human_input_mode=config.get('human_input_mode', \"TERMINATE\"),\n",
    "                function_map=config.get('function_map', {}),\n",
    "                code_execution_config=config.get('code_execution_config', False),\n",
    "                llm_config=llm_config,\n",
    "                default_auto_reply=config.get('default_auto_reply', \"\"),\n",
    "                description=config.get('description', \"\"),\n",
    "                chat_messages=config.get('chat_messages', {}),\n",
    "            )\n",
    "            self.avatar = config.get('avatar', 'ðŸ¤–')\n",
    "            self.verbose = config.get('verbose', False)\n",
    "            self.log_dir = config.get('log_dir', None)\n",
    "            self.enable_logs = config.get('enable_logs', True)\n",
    "            if self.enable_logs:\n",
    "                agent_name = config.get('name', 'DefaultAgentName')\n",
    "                current_date = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "                log_file_name = f\"{agent_name}_{current_date}.log\"\n",
    "                log_file_path = os.path.join(self.log_dir, log_file_name) if self.log_dir else log_file_name\n",
    "                self.logger = get_logger(log_file=log_file_path)\n",
    "            else:\n",
    "                self.logger = get_logger()\n",
    "        else:\n",
    "            self.avatar = avatar\n",
    "            self.verbose = verbose\n",
    "            self.enable_logs = enable_logs\n",
    "            self.log_dir = log_dir\n",
    "            if self.enable_logs:\n",
    "                agent_name = name if name else \"DefaultAgentName\"\n",
    "                current_date = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "                log_file_name = f\"{agent_name}_{current_date}.log\"\n",
    "                log_file_path = os.path.join(self.log_dir, log_file_name) if self.log_dir else log_file_name\n",
    "                self.logger = get_logger(log_file=log_file_path)\n",
    "            else:\n",
    "                self.logger = get_logger()\n",
    "\n",
    "            super().__init__(\n",
    "                name=name,\n",
    "                system_message=system_message,\n",
    "                is_termination_msg=is_termination_msg,\n",
    "                max_consecutive_auto_reply=max_consecutive_auto_reply,\n",
    "                human_input_mode=human_input_mode,\n",
    "                function_map=function_map,\n",
    "                code_execution_config=code_execution_config,\n",
    "                llm_config=llm_config,\n",
    "                default_auto_reply=default_auto_reply,\n",
    "                description=description,\n",
    "                chat_messages=chat_messages,\n",
    "            )\n",
    "\n",
    "        # Log the agent being loaded with all the information\n",
    "        self.logger.info(\"Agent loaded with the following configuration:\")\n",
    "\n",
    "        # Dynamically log all attributes of the client\n",
    "        for attribute, value in self.__dict__.items():\n",
    "            formatted_value = f\"\\n{value}\" if isinstance(value, dict) or isinstance(value, list) else value\n",
    "            self.logger.info(f\"    {attribute}: {formatted_value}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_agent_from_yaml(cls, yaml_file: str, llm_config_dict: Optional[Dict[str, Any]] = None) -> 'SuperConversableAgent':\n",
    "        \"\"\"\n",
    "        Load an agent configuration from a YAML file.\n",
    "\n",
    "        Args:\n",
    "            yaml_file (str): Path to the YAML configuration file.\n",
    "            llm_config_dict (Optional[Dict[str, Any]]): Custom LLM configuration dictionary.\n",
    "\n",
    "        Returns:\n",
    "            SuperConversableAgent: An instance of SuperConversableAgent.\n",
    "        \"\"\"\n",
    "        with open(yaml_file, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "    \n",
    "        return config\n",
    "    \n",
    "    # def receive(self, message: Dict[str, Any], sender: 'ConversableAgent', request_reply: bool = True, silent: bool = False):\n",
    "    #     \"\"\"\n",
    "    #     Receives a message, logs the interaction, and sends data to a remote database.\n",
    "    #     \"\"\"\n",
    "    #     try:\n",
    "    #         super().receive(message, sender, request_reply, silent)\n",
    "    #         timestamp = datetime.now().isoformat()\n",
    "    #         log_entry = {\n",
    "    #             \"timestamp\": timestamp,\n",
    "    #             \"action\": \"receive\",\n",
    "    #             \"sender\": sender.name,\n",
    "    #             \"receiver\": self.name,\n",
    "    #             \"message\": message\n",
    "    #         }\n",
    "    #         if self.enable_logs:\n",
    "    #             self.logger.info(log_entry)\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         self.logger.error(f\"Error processing message: {e}\")\n",
    "\n",
    "    # def send(self, message: Dict[str, Any], recipient: 'ConversableAgent'):\n",
    "    #     \"\"\"\n",
    "    #     Sends a message to a recipient, logs the interaction, and sends data to a remote database.\n",
    "    #     \"\"\"\n",
    "    #     try:\n",
    "    #         super().send(message, recipient)\n",
    "    #         timestamp = datetime.now().isoformat()\n",
    "    #         log_entry = {\n",
    "    #             \"timestamp\": timestamp,\n",
    "    #             \"action\": \"send\",\n",
    "    #             \"sender\": self.name,\n",
    "    #             \"receiver\": recipient.name,\n",
    "    #             \"message\": message\n",
    "    #         }\n",
    "    #         if self.logging:\n",
    "    #             self.logger.info(log_entry)\n",
    "        \n",
    "    #     except Exception as e:\n",
    "    #         self.logger.error(f\"Error sending message: {e}\")\n",
    "\n",
    "    def _process_received_message(self, message, sender, silent):\n",
    "        \"\"\"\n",
    "        Process received message and log it.\n",
    "\n",
    "        Args:\n",
    "            message (str): The message content.\n",
    "            sender (ConversableAgent): The sender of the message.\n",
    "            silent (bool, optional): Whether the message should be processed silently. Defaults to the instance's silent attribute.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().isoformat()\n",
    "            log_entry = {\n",
    "                \"timestamp\": timestamp,\n",
    "                \"action\": \"receive\",\n",
    "                \"sender\": sender.name,\n",
    "                \"receiver\": self.name,\n",
    "                \"message\": message\n",
    "            }\n",
    "            if self.logging:\n",
    "                self.logger.info(log_entry)\n",
    "\n",
    "            if self.verbose:\n",
    "                self._display_message(sender, message)\n",
    "\n",
    "            return super()._process_received_message(message, sender, silent)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing received message: {e}\")\n",
    "    \n",
    "    def _display_message(self, sender, message):\n",
    "        \"\"\"\n",
    "        Helper function to display a message in the chat interface.\n",
    "\n",
    "        Args:\n",
    "            sender (ConversableAgent): The sender of the message.\n",
    "            message (str): The message content.\n",
    "        \"\"\"\n",
    "        # Text-based formatting for the message\n",
    "        formatted_message = f\"Sender: {sender.name}\\nMessage: {message}\\n\"\n",
    "\n",
    "        # Print the formatted message to the console\n",
    "        print(formatted_message)\n",
    "\n",
    "        # Mapping agent names to emojis\n",
    "        agent_name_to_emoji = {\n",
    "            \"User\": \"ðŸ‘¤\",\n",
    "            \"MedicalResearchPlanner\": \"ðŸ§‘ðŸ¿â€ðŸ’¼\",\n",
    "            \"FinalMedicalReviewer\": \"ðŸ‘¨ðŸ½â€âš•ï¸\",\n",
    "            \"MedicalResearcher\": \"ðŸ‘©â€âš•ï¸\"\n",
    "        }\n",
    "\n",
    "        # Determine the avatar based on the sender's name\n",
    "        if sender.name in agent_name_to_emoji:\n",
    "            avatar = agent_name_to_emoji[sender.name]\n",
    "        else:\n",
    "            avatar = \"â“\"\n",
    "\n",
    "        # Print the avatar and formatted message\n",
    "        print(f\"{avatar} {formatted_message}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _resolve_llm_config(config: Dict[str, Any], llm_config_dict: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Resolve the LLM configuration based on the provided configuration and custom dictionary.\n",
    "\n",
    "        Args:\n",
    "            config (Dict[str, Any]): Configuration dictionary from the YAML file.\n",
    "            llm_config_dict (Dict[str, Any], optional): Custom LLM configuration dictionary.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Resolved LLM configuration.\n",
    "        \"\"\"\n",
    "        if config.get('llm_config') == \"default\":\n",
    "            return get_llm_config()\n",
    "        elif config.get('llm_config') == \"custom\":\n",
    "            return llm_config_dict if llm_config_dict else {}\n",
    "        return config.get('llm_config', {})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# researcher_agent = SuperConversableAgent(agent_from_yaml=r\"src\\app\\agents\\MedicalResearcher.yaml\")\n",
    "# planner_agent = SuperConversableAgent(agent_from_yaml=r\"src\\app\\agents\\MedicalResearchPlanner.yaml\")\n",
    "# reviewer_agent = SuperConversableAgent(agent_from_yaml=r\"src\\app\\agents\\MedicalReviewer.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = get_llm_config()\n",
    "\n",
    "medical_research_planner = SuperConversableAgent(\n",
    "    name=\"MedicalResearchPlanner\",\n",
    "    system_message=(\n",
    "        \"Given a research task, your role is to determine the specific information needed to comprehensively support the research. \"\n",
    "        \"You will assess the task's progress and delegate sub-tasks to other agents as needed. If no more information is needed and document looks good, mention STOP or TERMINATE\"\n",
    "    ),\n",
    "    llm_config=llm_config,\n",
    "    avatar=\"ðŸ“\"\n",
    ")\n",
    "\n",
    "reviewer_agent = SuperConversableAgent(\n",
    "    name=\"FinalMedicalReviewer\",\n",
    "    system_message=(\n",
    "        \"You are the final medical reviewer, tasked with aggregating and reviewing feedback from other reviewers. \"\n",
    "        \"Your role is to make the final decision on the content's readiness for publication, ensuring it adheres to all legal, \"\n",
    "        \"security, and ethical standards. If documentation is ready for public circulation, mention STOP or TERMINATE\"\n",
    "    ),\n",
    "    llm_config=llm_config,\n",
    "    avatar=\"ðŸ”\"\n",
    ")\n",
    "\n",
    "researcher_agent = SuperConversableAgent(\n",
    "    name=\"MedicalResearcher\",\n",
    "    system_message=(\n",
    "        \"As a Medical Researcher, your role is to draft a comprehensive manuscript detailing your study's findings. \"\n",
    "        \"Ensure the manuscript is scientifically robust, covering all critical aspects of your research.\"\n",
    "    ),\n",
    "    llm_config=llm_config,\n",
    "    avatar=\"ðŸ”¬\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import GroupChat, GroupChatManager\n",
    "import autogen\n",
    "\n",
    "agents_dict = {\n",
    "    \"FinalMedicalReviewer\": reviewer_agent,\n",
    "    \"MedicalResearcher\": researcher_agent,\n",
    "}\n",
    "\n",
    "\n",
    "# Define the group chat for the medical use case with limited agents\n",
    "medical_groupchat = GroupChat(\n",
    "    agents=[\n",
    "        agents_dict[\"FinalMedicalReviewer\"], \n",
    "        agents_dict[\"MedicalResearcher\"]\n",
    "    ],\n",
    "    messages=[],\n",
    "    max_round=3,\n",
    "    allowed_or_disallowed_speaker_transitions={\n",
    "        agents_dict[\"FinalMedicalReviewer\"]: [\n",
    "            agents_dict[\"MedicalResearcher\"]\n",
    "        ],\n",
    "        agents_dict[\"MedicalResearcher\"]: [\n",
    "            agents_dict[\"FinalMedicalReviewer\"]\n",
    "        ]\n",
    "    },\n",
    "    speaker_transitions_type=\"allowed\",\n",
    ")\n",
    "\n",
    "# Initialize the GroupChatManager with the medical group chat and LLM configuration\n",
    "medical_manager = GroupChatManager(\n",
    "    groupchat=medical_groupchat, \n",
    "    llm_config=get_llm_config(),\n",
    "    system_message=\"\"\" \n",
    "    - \"Your role is to orchestrate the entire research process for a given task.\"\n",
    "    - \"Determine the specific information needed to comprehensively support the research.\"\n",
    "    - \"Assess the task's progress and delegate sub-tasks to other agents as needed.\"\n",
    "    - \"Collaborate with the MedicalResearcher to draft a comprehensive manuscript.\"\n",
    "    - \"Work with the MedicalReviewer to ensure the manuscript meets all legal, security, and ethical standards.\"\n",
    "    - \"Once all necessary information is gathered and the document is ready, clearly conclude the task by mentioning STOP or TERMINATE.\"\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: df4c1b1c-fc08-483a-af68-ecebe02e9856\n",
      "Logging session ID: 00a02365-918d-4443-a57b-5331c1bbd3c2\n",
      "\u001b[33mMedicalResearcher\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "        Create a comprehensive medical research document detailing the study on \n",
      "        the effects of a new drug on heart disease. Include methodology, results, \n",
      "        discussion, and conclusion sections. Ensure the document adheres to medical \n",
      "        research standards and ethical guidelines. Please support your findings with\n",
      "        relevant references from PubMed.\n",
      "       \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m logging_session_id \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mruntime_logging\u001b[38;5;241m.\u001b[39mstart(logger_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.log\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogging session ID: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(logging_session_id))\n\u001b[1;32m---> 31\u001b[0m chat_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m researcher_agent\u001b[38;5;241m.\u001b[39ma_initiate_chat(\n\u001b[0;32m     32\u001b[0m     recipient\u001b[38;5;241m=\u001b[39mmedical_manager,\n\u001b[0;32m     33\u001b[0m     message\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m     34\u001b[0m     max_turns\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     35\u001b[0m     is_termination_msg\u001b[38;5;241m=\u001b[39mis_termination_msg\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m autogen\u001b[38;5;241m.\u001b[39mruntime_logging\u001b[38;5;241m.\u001b[39mstop()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Print the final document result\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\agentic-framework\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1077\u001b[0m, in \u001b[0;36mConversableAgent.a_initiate_chat\u001b[1;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[0;32m   1075\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m msg2send \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1076\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1077\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_send(msg2send, recipient, request_reply\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39msilent)\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\agentic-framework\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:705\u001b[0m, in \u001b[0;36mConversableAgent.a_send\u001b[1;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[0;32m    703\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[1;32m--> 705\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m recipient\u001b[38;5;241m.\u001b[39ma_receive(message, \u001b[38;5;28mself\u001b[39m, request_reply, silent)\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    709\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\agentic-framework\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:855\u001b[0m, in \u001b[0;36mConversableAgent.a_receive\u001b[1;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 855\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_generate_reply(sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_send(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\agentic-framework\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:2042\u001b[0m, in \u001b[0;36mConversableAgent.a_generate_reply\u001b[1;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m   2041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39miscoroutinefunction(reply_func):\n\u001b[1;32m-> 2042\u001b[0m         final, reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m reply_func(\n\u001b[0;32m   2043\u001b[0m             \u001b[38;5;28mself\u001b[39m, messages\u001b[38;5;241m=\u001b[39mmessages, sender\u001b[38;5;241m=\u001b[39msender, config\u001b[38;5;241m=\u001b[39mreply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2044\u001b[0m         )\n\u001b[0;32m   2045\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2046\u001b[0m         final, reply \u001b[38;5;241m=\u001b[39m reply_func(\u001b[38;5;28mself\u001b[39m, messages\u001b[38;5;241m=\u001b[39mmessages, sender\u001b[38;5;241m=\u001b[39msender, config\u001b[38;5;241m=\u001b[39mreply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\agentic-framework\\lib\\site-packages\\autogen\\agentchat\\groupchat.py:1133\u001b[0m, in \u001b[0;36mGroupChatManager.a_run_chat\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39ma_select_speaker(speaker, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[1;32m-> 1133\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m speaker\u001b[38;5;241m.\u001b[39ma_generate_reply(sender\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;66;03m# let the admin agent speak if interrupted\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39madmin_name \u001b[38;5;129;01min\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39magent_names:\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;66;03m# admin agent is one of the participants\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\agentic-framework\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:2046\u001b[0m, in \u001b[0;36mConversableAgent.a_generate_reply\u001b[1;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[0;32m   2042\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m reply_func(\n\u001b[0;32m   2043\u001b[0m         \u001b[38;5;28mself\u001b[39m, messages\u001b[38;5;241m=\u001b[39mmessages, sender\u001b[38;5;241m=\u001b[39msender, config\u001b[38;5;241m=\u001b[39mreply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2044\u001b[0m     )\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2046\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[0;32m   2048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\agentic-framework\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:1719\u001b[0m, in \u001b[0;36mConversableAgent.check_termination_and_human_reply\u001b[1;34m(self, messages, sender, config)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1718\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender] \u001b[38;5;28;01mif\u001b[39;00m sender \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m-> 1719\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1720\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1721\u001b[0m no_human_input_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Define the task for the initial content generation\n",
    "task = '''\n",
    "        Create a comprehensive medical research document detailing the study on \n",
    "        the effects of a new drug on heart disease. Include methodology, results, \n",
    "        discussion, and conclusion sections. Ensure the document adheres to medical \n",
    "        research standards and ethical guidelines. Please support your findings with\n",
    "        relevant references from PubMed.\n",
    "       '''\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "terminate_pattern = re.compile(r\".*STOP.*\", re.IGNORECASE)\n",
    "terminate_variation_pattern = re.compile(r\".*TERMINATED.*\", re.IGNORECASE)\n",
    "finalize_pattern = re.compile(r\".*TERMINATE.*\", re.IGNORECASE)\n",
    "\n",
    "is_termination_msg = lambda x: any([\n",
    "    terminate_pattern.search(x.get(\"content\", \"\")),\n",
    "    terminate_variation_pattern.search(x.get(\"content\", \"\")),\n",
    "    finalize_pattern.search(x.get(\"content\", \"\")),\n",
    "])\n",
    "\n",
    "# Generate a unique ID for the run\n",
    "run_id = str(uuid.uuid4())\n",
    "print(\"Run ID: \" + run_id)\n",
    "\n",
    "# Start logging with the run ID\n",
    "logging_session_id = autogen.runtime_logging.start(logger_type=\"file\", config={\"filename\": f\"run_{run_id}.log\"})\n",
    "print(\"Logging session ID: \" + str(logging_session_id))\n",
    "\n",
    "chat_result = await researcher_agent.a_initiate_chat(\n",
    "    recipient=medical_manager,\n",
    "    message=task,\n",
    "    max_turns=2,\n",
    "    is_termination_msg=is_termination_msg\n",
    ")\n",
    "autogen.runtime_logging.stop()\n",
    "\n",
    "# Print the final document result\n",
    "print(\"Final Document:\", chat_result.chat_history[-1]['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
